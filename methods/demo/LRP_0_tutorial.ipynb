{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### LRP-0 tutorial\n",
    "Contribution Analysis aims to explain the decision-making progress of a neuron network.\n",
    "\n",
    "Take the composed function $f$ represents the NN. Given one sample input $X=\\{X_i\\}_i$, $f$ output the scalar score $f(X)$.\n",
    "\n",
    "We should decompose the output into the contribution of each input $R(X_i)$.\n",
    "\n",
    "**LRP** propose the first property, **conservation**, that is:\n",
    "\n",
    "$$f(X)=\\sum_i R(X_i)$$\n",
    "\n",
    "It means the summation of all contributions $R(X_i)$ equals to the output itself.\n",
    "\n",
    "Here is an easy example for the conservation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Rx equals to the output y? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.rand(1,5)\n",
    "y = X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "It easy to find the contribution of each X. It is X itself."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Rx = X\n",
    "print(f'Sum of Rx equals to the output y? {Rx.sum()==y}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, the contribution is the same as the input itself.\n",
    "\n",
    "When coming to the complex situation, **LRP-0** gives us a reasonable answer.\n",
    "\n",
    "LRP-0 applies only for the stacked linear NN. A Linear Network is as:\n",
    "\n",
    "$$Y^l=W^l*Y^{l-1}+B^l, \\quad l=1...L$$\n",
    "\n",
    "$W^l$ and $B^l$ is the weight and the bias of layer $l$.\n",
    "\n",
    "$Y^l$ is the output of layer $l$.\n",
    "\n",
    "Specifically, $Y^0=X$\n",
    "\n",
    "The l travels from 1 to L that means the input of each layer is the output before."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### scalar linear\n",
    "First take an easy example of one linear layer with dim of y is one and no bias."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = torch.randn(3)\n",
    "W = torch.randn(1,3)\n",
    "y = W.mm(X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Suppose Ry = y, then we want to get R(X)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Ry = y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We separate the dimension of X and y by reshaping. dim_y in left side."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = X.reshape(1,3)\n",
    "y = y.reshape(1,1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These gives a simple insight to forward."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print((X*W).sum(dim=1))\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It easy to find the contribution of each X. It is weighted X."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Rx = X * W\n",
    "print(f'Sum of Rx equals to the output y? {Rx.sum()==y}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ry surely can be any with the shape like y. Then What is R(X)?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "here is a general lrp-0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def lrp_0(layer, x, Ry):\n",
    "    # -- Original Relevance Propagation\n",
    "    # This demo is to demonstrate how the relevance propagate.\n",
    "    # If you use this jacobian version, will get \"CUDA out of memory\" in common CNNs.\n",
    "    # we move dim of x to the end, y to the start. dim of x&y is seperated.\n",
    "    x = x.squeeze(0)  # remove batch dim\n",
    "    Ry = Ry.squeeze(0)\n",
    "    x_dim_depth = len(x.shape)\n",
    "    x_empty_dim = (1,) * x_dim_depth\n",
    "    y = layer.forward(x)\n",
    "    y_dim_depth = len(y.shape)\n",
    "    # unnecessary to reshape x, because pytorch ignore the front empty dimension.\n",
    "    y = y.reshape(y.shape + x_empty_dim)  # y as (y_shape, 1,..,1)\n",
    "    # we get the jacobian whose dim match x&y\n",
    "    # on FC layer , you will see jacobian == layer.weight\n",
    "    g = torch.autograd.functional.jacobian(lambda x: layer.forward(x), x)\n",
    "    # we use jacobian to approximate the increment of output\n",
    "    r = safeDivide(Ry * g * x, y)\n",
    "    Rx = r.sum(list(range(y_dim_depth)))  # sum according y_shape\n",
    "    return Rx.unsqueeze(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
